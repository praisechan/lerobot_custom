# Pi0Inference CUDA MPS - Quick Reference Card

## ğŸ¯ One-Liner Examples

```python
# Basic (default 50-50 concurrent)
output = infer.forward_mps(image, state, noise, concurrent=True)

# Encoder-heavy
output = infer.forward_mps(image, state, noise, mps_encoder_percentage=70, concurrent=True)

# Decoder-heavy  
output = infer.forward_mps(image, state, noise, mps_encoder_percentage=30, concurrent=True)

# Sequential
output = infer.forward_mps(image, state, noise, concurrent=False)

# Original (CUDA graphs)
output = infer.forward(image, state, noise, concurrent=True)
```

## ğŸ“Š Method Comparison Matrix

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Method               â”‚ Execution Mode     â”‚ SM Control           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ forward()            â”‚ CUDA Graphs        â”‚ Implicit             â”‚
â”‚ forward_mps()        â”‚ Live code          â”‚ Explicit (50-50 def) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Use forward() for:          Use forward_mps() for:
â”œâ”€ Maximum performance      â”œâ”€ Resource tuning
â”œâ”€ Real-time latency        â”œâ”€ Load balancing
â””â”€ CUDA graph replay        â””â”€ Experimentation
```

## ğŸ”§ Parameter Reference

```python
def forward_mps(
    self, 
    observation_images_normalized,        # Tensor [num_views, 224, 224, 3]
    observation_state_normalized,         # Tensor [32]
    diffusion_noise,                      # Tensor [chunk_size, 32]
    mps_encoder_percentage=50,            # 1-100, default 50
    mps_decoder_percentage=50,            # 1-100, default 50
    concurrent=False                      # bool, default False
)
```

## âš¡ Performance Tuning

```
Balanced (50-50) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â”œâ”€ Good starting point
  â””â”€ Equal load assumption

Encoder-Heavy (70-30) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â”œâ”€ When encoder is bottleneck
  â””â”€ Try: 60-40, 70-30, 80-20

Decoder-Heavy (30-70) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â”œâ”€ When decoder is bottleneck
  â””â”€ Try: 40-60, 30-70, 20-80

Sequential (100-100) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â”œâ”€ Baseline measurement
  â””â”€ Each stage gets full resources
```

## ğŸ“ˆ Quick Benchmark

```bash
# Run all configurations
python benchmark_forward_methods.py --iterations 20

# Output shows:
# - Fastest method
# - Performance difference vs baseline
# - Recommendations
```

## ğŸ“ Learning Path

```
1. Read README_MPS.md (5 min)
   â””â”€ Get overview

2. Run pi0_infer_mps_quickstart.py (2 min)
   â””â”€ See it work

3. Read CUDA_MPS_GUIDE.md (15 min)
   â””â”€ Understand details

4. Run pi0_infer_mps_example.py (10 min)
   â””â”€ Explore configurations

5. Run benchmark_forward_methods.py (20 min)
   â””â”€ Optimize for your hardware

6. Profile with NSys (5 min)
   â””â”€ Visualize execution timeline
```

## ğŸ› Troubleshooting

| Problem | Solution |
|---------|----------|
| MPS allocation not taking effect | Add `torch.cuda.synchronize()` before/after change |
| Concurrent slower than sequential | Try different split (70-30, 80-20) |
| GPU not supported | Check GPU is Maxwell or newer (GTX 750+) |
| Import errors | Ensure pi0_infer.py is in Python path |

## ğŸ“Š Benchmark Configurations

```
1. forward() - Sequential
2. forward() - Concurrent (CUDA Graphs)  â† Baseline
3. forward_mps() - Sequential (100-100)
4. forward_mps() - Concurrent (50-50)     â† Balanced
5. forward_mps() - Concurrent (70-30)     â† Encoder-heavy
6. forward_mps() - Concurrent (30-70)     â† Decoder-heavy
```

## ğŸ¯ Decision Tree

```
Do you need maximum performance?
â”œâ”€ YES â†’ Use forward(concurrent=True)
â””â”€ NO â†’ Continue

Do you want to tune SM allocation?
â”œâ”€ YES â†’ Use forward_mps(concurrent=True)
â””â”€ NO â†’ Use forward(concurrent=True)

Is one component slower?
â”œâ”€ YES â†’ Use forward_mps() with custom split
â””â”€ NO â†’ Use balanced 50-50 split
```

## ğŸ”— File Quick Links

| File | Purpose | Read Time |
|------|---------|-----------|
| [README_MPS.md](README_MPS.md) | Package overview | 5 min |
| [MPS_IMPLEMENTATION_SUMMARY.md](MPS_IMPLEMENTATION_SUMMARY.md) | How it works | 10 min |
| [CUDA_MPS_GUIDE.md](CUDA_MPS_GUIDE.md) | Complete reference | 20 min |
| [COMPARISON_forward_vs_forward_mps.py](COMPARISON_forward_vs_forward_mps.py) | Comparison guide | 10 min |
| [pi0_infer_mps_quickstart.py](pi0_infer_mps_quickstart.py) | Minimal examples | 1 min |
| [pi0_infer_mps_example.py](pi0_infer_mps_example.py) | Full examples | 5 min |
| [benchmark_forward_methods.py](benchmark_forward_methods.py) | Benchmarks | 20 min |

## ğŸš€ Common Workflows

### Workflow 1: Get Started (5 minutes)
```bash
python pi0_infer_mps_quickstart.py
# Done! You've seen all basic patterns
```

### Workflow 2: Understand Differences (20 minutes)
```bash
python pi0_infer_mps_example.py --iterations 10
# Saw different configurations
# Then read CUDA_MPS_GUIDE.md
```

### Workflow 3: Optimize for Hardware (30 minutes)
```bash
python benchmark_forward_methods.py --iterations 50
# Got detailed performance comparison
# Ready to choose best configuration
```

### Workflow 4: Profile Execution (15 minutes)
```bash
nsys profile -o output python pi0_infer_mps_example.py
nsys-ui output.nsys-rep
# Visualized execution timeline
# Identified optimization opportunities
```

## ğŸ’¡ Pro Tips

1. **Always warmup first**: GPU needs a few iterations to reach optimal clocks
2. **Use NVTX markers**: Built into examples for easy NSys profiling
3. **Try different splits**: What works on your GPU may differ from others
4. **Monitor memory**: Check memory bandwidth for contention
5. **Save optimal config**: Once tuned, hardcode the percentages

## ğŸ“ˆ Expected Performance

```
Configuration          | Latency vs forward()  | Flexibility
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
forward() concurrent   | 0% (baseline)        | None
forward_mps() 50-50    | +5-15%               | High
forward_mps() 70-30    | +5-12% (tuned)       | High
forward_mps() 30-70    | +8-15% (tuned)       | High
```

## âœ… Verification Checklist

- [ ] Read README_MPS.md
- [ ] Run quickstart.py
- [ ] Run example.py
- [ ] Run benchmark.py
- [ ] Read CUDA_MPS_GUIDE.md
- [ ] Profile with NSys
- [ ] Choose optimal configuration
- [ ] Integrate into production

---

**Status**: Ready to Use âœ…  
**Last Updated**: 2026-02-05  
**Questions?** Check CUDA_MPS_GUIDE.md or run the examples!
